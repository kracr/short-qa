{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a84c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import pickle\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8eb4c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step away from the buzz and bustle of De  [...53059 chars more]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "with open(\"./data/humayun_text\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(f\"{text[:40]}  [...{len(text)-40} chars more]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee451bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 568 sentences\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text) \n",
    "print(f\"Found {len(sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df2db3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the buzz: {0}\n",
      "bustle: {0}\n",
      "delhi: {0, 98, 450, 452, 39, 264, 456, 49, 306, 95}\n",
      "crowded roads: {0}\n",
      "neighbourhoods: {0}\n",
      "the tranquil oasis: {0}\n",
      "humayunâ€™s tomb: {0, 355, 101, 261, 383, 271, 144, 178, 83, 19, 181, 437, 280, 25, 31}\n",
      "the nizamuddin east area: {0}\n",
      "it: {1, 514, 8, 10, 526, 529, 530, 19, 22, 28, 29, 32, 544, 545, 546, 85, 102, 109, 116, 117, 119, 130, 137, 141, 149, 151, 153, 159, 168, 169, 193, 196, 212, 213, 214, 215, 216, 222, 224, 233, 240, 248, 259, 267, 271, 277, 279, 290, 318, 322, 330, 353, 355, 359, 360, 361, 364, 376, 381, 388, 395, 398, 410, 420, 424, 425, 429, 430, 437, 438, 440, 444, 449, 450, 456, 457, 471, 483, 485, 499, 508, 511}\n",
      "a unesco world heritage site: {1}\n",
      "[...1603 more]\n",
      "\n",
      "Saw 561 sentences and 1603 entities!\n"
     ]
    }
   ],
   "source": [
    "sentences_map = {} # sentence id -> sentence_text\n",
    "seen_sentences = set() # seen set\n",
    "\n",
    "graph = {} # entity -> set(sentence ids)\n",
    "\n",
    "idx = 0\n",
    "for sentence in sentences:\n",
    "    \n",
    "    sentence = ' '.join(sentence.split()).lower()\n",
    "    if sentence in seen_sentences:\n",
    "        continue\n",
    "\n",
    "    curr_idx = idx\n",
    "    \n",
    "    sentences_map[curr_idx] = sentence\n",
    "    seen_sentences.add(sentence)\n",
    "    idx += 1\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk = chunk.text.lower()\n",
    "        if chunk in graph:\n",
    "            graph[chunk].add(curr_idx)\n",
    "        else:\n",
    "            graph[chunk] = set((curr_idx, ))\n",
    "\n",
    "for entity, linked_sentences in tuple(graph.items())[:10]:\n",
    "    print(f\"{entity}: {linked_sentences}\")\n",
    "print(f\"[...{len(graph)} more]\")    \n",
    "\n",
    "print(f\"\\nSaw {len(seen_sentences)} sentences and {len(graph)} entities!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc77e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to './data/humayun_ir_entity_graph.pkl' ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./data/humayun_ir_entity_graph.pkl\"\n",
    "with open(save_path, \"wb\") as file:\n",
    "    pickle.dump((graph, sentences_map), file)\n",
    "print(f\"Saved to '{save_path}' ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4489ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from './data/humayun_ir_entity_graph.pkl' - 561 sentences and 1603 entities âœ…\n"
     ]
    }
   ],
   "source": [
    "read_graph, read_sentences_map = None, None\n",
    "with open(save_path, \"rb\") as file:\n",
    "    read_graph, read_sentences_map = pickle.load(file)\n",
    "\n",
    "print(f\"Loaded from '{save_path}' - {len(read_sentences_map)} sentences and {len(read_graph)} entities âœ…\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
